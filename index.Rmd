---
title: "Practical Machine Learning - Course Project"
author: "Gabriel"
date: "12/11/2021"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE,warning = FALSE, message = FALSE, cache = TRUE)
options(scipen=999)
```

## Introduction

Using devices such as Jawbone Up, Nike FuelBand, and Fitbit it is now possible to collect a large amount of data about personal activity relatively inexpensively. These type of devices are part of the quantified self movement â€“ a group of enthusiasts who take measurements about themselves regularly to improve their health, to find patterns in their behavior, or because they are tech geeks. One thing that people regularly do is quantify how much of a particular activity they do, but they rarely quantify how well they do it. The data for this project come from this source: http://groupware.les.inf.puc-rio.br/har.

The training data for this project were downloaded from: 
https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv

The test data were downloaded from:
https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv

The goal of this project is to use data from accelerometers on the belt, forearm, arm, and dumbell of 6 participants. The participants were asked to perform barbell lifts correctly and incorrectly in 5 different ways. This is the "classe" variable in the training set and the other measurements will be used to predict classe. The report will describe the preprocessing, cross validation and model building used to make the prediction. In the end, we will choose the machine learning algorithm with the highest accuracy to apply to the test data.


## Model building and selection

First, we will download the .csv files for the training and test data and create two data frames.

```{r}
library(ggplot2)
library(lattice)
library(caret)
library(rattle)

train_URL <-"http://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv"
test_URL <- "http://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv"

if (!file.exists("pml-training.csv")) {
  download.file(train_URL, destfile = "pml-training.csv", method = "curl")
}

train_data <- read.csv("pml-training.csv", header = TRUE)

if (!file.exists("pml-testing.csv")) {
  download.file(test_URL, destfile = "pml-testing.csv", method = "curl")
}

test_data <- read.csv("pml-testing.csv", header = TRUE)
```

The first step of building the model is to inspect the training data and remove any variables that we will not use in the final model.

```{r}
head(train_data[1:7],5)
train_data <- train_data[,-c(1:7)]
```

The first 7 variables contain timestamps, usernames and other data that should not have an effect on the outcome of our prediction so we can safely remove them. 

```{r}
dim(train_data)
colSums(is.na(train_data))[colSums(is.na(train_data))!=0]
```

There are `r dim(train_data)[2]` variables and `r dim(train_data)[1]` observations. First, we look for missing values. The table above shows all the variables that have at least 1 missing value. It looks like all of them have `r unique(colSums(is.na(train_data))[colSums(is.na(train_data))!=0])` missing values. This is close to the total number of observations so we can completely remove these variables.

```{r}
train_data <- train_data[, colSums(is.na(train_data)) == 0]
nsv <- nearZeroVar(train_data)
train_data <- train_data[,-nsv]
```

We will also remove variables with near zero variability. This leaves us with `r dim(train_data)[2]-1` predictors plus the response variable "classe".

One final step before cross-validation is to convert the "classe" variable to factor, which is very important for our model building.

```{r}
train_data$classe <- as.factor(train_data$classe)
```

For cross-validation, we will split the training data into the actual training and test sets using an 80/20 split as we have a fairly large data set. We also set a seed so that the results can be reproduced.

```{r}
set.seed(452)
inTrain <- createDataPartition(y=train_data$classe, p=0.8, list=F)
train <- train_data[inTrain,]
test <- train_data[-inTrain,]
```

Next, we will be building 3 models as follows:

* Decision Tree
* Bagging Method
* Random Forests

For all three models, we will apply a 5-fold cross validation using the trControl argument. A k=5 should be sufficient for a balance between bias and variance. We will also standardize the data using the preProcess argument with the parameters "center" and "scale".

After training each model, we will predict using the test set from the 80/20 split and create a confusion matrix. This will produce a value for Accuracy, which is 1 minus the out of sample error. Therefore, the higher the accuracy, the better the model is.

The first model we train is the Decision Tree using Recursive Partitioning in R.

```{r}
modRpart <- train(classe~., data=train, method="rpart", trControl = trainControl(method="cv",number=5) ,preProcess=c("center", "scale"))
fancyRpartPlot(modRpart$finalModel)
predRpart <- predict(modRpart, test)
cmRpart <- confusionMatrix(predRpart, test$classe)
cmRpart
```

The second model is a bagging method using "treebag" in R.

```{r}
modBag <- train(classe~.,data=train, method = "treebag", trControl = trainControl(method="cv",number=5), preProcess=c("center", "scale"))
predBag <- predict(modBag, test)
cmBag <- confusionMatrix(predBag, test$classe)
cmBag
```

The third and final model is the Random Forests.

```{r}
modRf <- train(classe~., data=train, method="rf", trControl = trainControl(method="cv",number=5), preProcess=c("center", "scale"))
predRf <- predict(modRf, test)
cmRf <- confusionMatrix(predRf, test$classe)
cmRf
```

```{r}
M <- matrix(c(cmRpart$overall[1],cmBag$overall[1],cmRf$overall[1]), ncol=3)
colnames(M)<-c("Trees","Bagging","Random Forest")
rownames(M) <- "Accuracy"
M
```

Out of the 3 models, Random Forests has the highest accuracy of `r round(cmRf$overall[1],3)`. As the model with the highest accuracy and therefore lowest out of sample error, we will choose this for our prediction. We could test other models but given the very high value of almost 1, we are not likely to find something much better.

## Applying model on test data

If we apply the Random Forests model on the original test data with `r dim(test_data)[1]` observations, we will get the following outcomes for the classe variable:

```{r}
test_data <- test_data[,-c(1:7)]
test_data <- test_data[, colSums(is.na(test_data)) == 0]
pred <- predict(modRf, test_data)
pred
```

